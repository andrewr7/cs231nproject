{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regular_model_looper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOM0VmwUTVe2",
        "cellView": "form",
        "outputId": "18e87efd-7c4e-41b2-a161-a463d23118de"
      },
      "source": [
        "#@title Import all the libraries we need ####################################################\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Data Reading\n",
        "from skimage.io import imread, imsave\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "# TensorFlow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "import h5py\n",
        "from IPython.display import Image\n",
        "!pip install ipynb\n",
        "\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/31/42/4c0bbb66390e3a68e04ebf134c8d074a00c18b5882293f8ace5f7497fbf0/ipynb-0.5.1-py3-none-any.whl\n",
            "Installing collected packages: ipynb\n",
            "Successfully installed ipynb-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SRNMsYroTkBz",
        "outputId": "5578a60f-3433-4091-be5d-8a7a147d625c"
      },
      "source": [
        "#@title download files from google drive into colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #after you run this a link will pop up for you to click"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWRLBaSmToLx",
        "cellView": "form"
      },
      "source": [
        "#@title Function Cell ###############################################\n",
        "\n",
        "\n",
        "# keypoints function\n",
        "def pklpath2tensor(pkl_file_path):\n",
        "  # open a file, where you stored the pickled data\n",
        "  file = open(pkl_file_path, 'rb')\n",
        "  # dump information to that file\n",
        "  data = pickle.load(file)\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  keypoints3d = data['keypoints3d']\n",
        "  keypoints3d_5Hz = keypoints3d[0::12,:,:]\n",
        "  keypoints_tensor = tf.convert_to_tensor(keypoints3d_5Hz)#, dtype=None, dtype_hint=None, name=None)\n",
        "  return keypoints_tensor\n",
        "\n",
        "def vidpath2picklepath(folder, filename):\n",
        "  string = os.path.basename(os.path.normpath(filename))\n",
        "  for substring in [\"_c01_\",\"_c02_\",\"_c03_\",\"_c04_\",\"_c05_\",\"_c06_\",\"_c07_\",\"_c08_\", \"_c09_\"]:\n",
        "    string = string.replace(substring,\"_cAll_\")\n",
        "  return folder + string + \".pkl\"\n",
        "\n",
        "def image2tensor(file_path):\n",
        "  \"\"\"\n",
        "  Given an image file path returns a tensor of the image\n",
        "  \"\"\"\n",
        "  # Reads image\n",
        "  image = imread(file_path)\n",
        "  # Downsamples by 4 to go from 1080x1920 to 270x480\n",
        "  resized = image[::4, ::4, :]\n",
        "  # Converts numpy array to tensor\n",
        "  tensor = tf.convert_to_tensor(resized, dtype=tf.float32)\n",
        "  # Shape is (270, 480, 3)\n",
        "  return tensor\n",
        "\n",
        "def check_num_frames(frame_tensor, keypoint_tensor):\n",
        "  if keypoint_tensor.shape[0] < frame_tensor.shape[0]:\n",
        "    num_to_cut = frame_tensor.shape[0] - keypoint_tensor.shape[0] \n",
        "    frame_tensor = frame_tensor[0:-num_to_cut,:,:,:]\n",
        "  elif keypoint_tensor.shape[0] > frame_tensor.shape[0]:\n",
        "    num_to_cut = - frame_tensor.shape[0] + keypoint_tensor.shape[0]\n",
        "    keypoint_tensor = frame_tensor[0:-num_to_cut,:,:]\n",
        "  return frame_tensor, keypoint_tensor\n",
        "\n",
        "# Read file contents and put each line into a list\n",
        "def read_files_from_list(text_destination):\n",
        "  dis_list = []\n",
        "  with open(text_destination, 'r') as f:\n",
        "    for line in f:\n",
        "      nonewline = line[:-1]       # Removes the \\n from the end of each line\n",
        "      dis_list.append(nonewline)  # Add file to list\n",
        "  return dis_list\n",
        "\n",
        "# Write and create a text file that prints some statement (will overwrite if file exists)\n",
        "def write_list_to_file(dis_list, file_path):\n",
        "  with open(file_path, 'w') as f:\n",
        "    for item in dis_list:\n",
        "      print(item, file=f)\n",
        "\n",
        "\n",
        "def update_vids_converted_file(new_vids, file_path = \"/content/drive/My Drive/CS230Project/data/vids_converted_to_data.txt\"):\n",
        "  f = open(file_path, \"a\")\n",
        "  f.close()\n",
        "  vid_list_old = read_files_from_list(file_path)\n",
        "  vid_list = vid_list_old + new_vids\n",
        "  write_list_to_file(vid_list, file_path)\n",
        "\n",
        "\n",
        "def get_list_of_frames(images_folder, labels_folder, visited_list, num_videos = 200,\n",
        "  visited_list_file_destination = \"/content/drive/My Drive/CS230Project/framesInfo/converted_tensor_list.txt\"):\n",
        "  first_tensor = True\n",
        "  first_vid = True\n",
        "  j = 0\n",
        "  frame_tensor_list = []\n",
        "  keypoints_list = []\n",
        "  root_list = []\n",
        "\n",
        "  for root, dirs, files in os.walk(images_folder, topdown=True): \n",
        "    if root in visited_list:\n",
        "      continue\n",
        "    elif j == 0:\n",
        "      j += 1\n",
        "      # Skip the first iteration of os.walk since it starts with the root folder\n",
        "      continue\n",
        "    elif j % 10 == 0:\n",
        "      print(f\"finished {j} iterations\")\n",
        "    elif j > num_videos:\n",
        "      break\n",
        "    j += 1\n",
        "\n",
        "    # Add to visited list\n",
        "    visited_list.append(root)\n",
        "    root_list.append(root)\n",
        "\n",
        "    #pickle stuff\n",
        "    pickle_file = vidpath2picklepath(labels_folder, root)\n",
        "    keypoints_list.append(pklpath2tensor(pickle_file))\n",
        "\n",
        "    for file in files:\n",
        "      # Get the tensor from the file given in the root path\n",
        "      frame_tensor = image2tensor(root + \"/\" + file)\n",
        "\n",
        "      if first_tensor:\n",
        "        first_tensor = False   # Toggle boolean so this block only runs once\n",
        "\n",
        "        # Makes the first tensor of dimension (1, length, width, channel)\n",
        "        all_tensor_frames = tf.expand_dims(frame_tensor, axis=0)\n",
        "\n",
        "      else:\n",
        "        # Makes the next tensor frame of dimension (1, length, width, channel)\n",
        "        next_tensor_frame = tf.expand_dims(frame_tensor, axis=0)\n",
        "        # Concatenates onto the rest of the tensors\n",
        "        all_tensor_frames = tf.concat([all_tensor_frames, next_tensor_frame], axis=0)\n",
        "\n",
        "    #add tensor to tensor list\n",
        "    frame_tensor_list.append(all_tensor_frames / 255.0)\n",
        "    first_tensor = True\n",
        "\n",
        "  # match the frame number for each video\n",
        "  print(\"checking\")\n",
        "  for i in range(len(frame_tensor_list)):\n",
        "    frame_tensor_list[i] ,keypoints_list[i] = check_num_frames(frame_tensor_list[i], keypoints_list[i])\n",
        "\n",
        "  # Write the visited list to a file for long term storage\n",
        "  write_list_to_file(visited_list, visited_list_file_destination)\n",
        "  update_vids_converted_file(root_list)\n",
        "  return frame_tensor_list , keypoints_list, root_list\n",
        "\n",
        "def savedata2file(frame_tensor_list, keypoints_list, root_list, file_path = '/content/drive/My Drive/CS230Project/data/MEGADATAFILE.hdf5'):\n",
        "  with h5py.File(file_path, 'a') as hf:\n",
        "    for i in range(len(root_list)):\n",
        "      grp = hf.create_group(root_list[i])\n",
        "      grp.create_dataset('frames',  data = frame_tensor_list[i])\n",
        "      grp.create_dataset('keypoints', data = keypoints_list[i])\n",
        "\n",
        "\n",
        "#### old get samples\n",
        "def get_samples_h5py(h5py_path, num_videos, finished_samples, vid_name_list_path = \"/content/drive/My Drive/CS230Project/data/vids_converted_to_data.txt\"):\n",
        "  vid_name_list = read_files_from_list(vid_name_list_path)\n",
        "  vid_name_list = vid_name_list[finished_samples:]\n",
        "  first_iter = True\n",
        "  with h5py.File(h5py_path, 'r') as hf:\n",
        "    for i, video in enumerate(vid_name_list):\n",
        "      if i >= num_videos:\n",
        "        break\n",
        "      if first_iter:\n",
        "        image_set = hf[video + '/frames'][:]\n",
        "        keypoint_set = hf[video + '/keypoints'][:]\n",
        "        first_iter = False\n",
        "      else:\n",
        "        image_set = tf.concat([image_set, hf[video + '/frames'][:]], axis = 0)\n",
        "        keypoint_set = tf.concat([keypoint_set, hf[video + '/keypoints'][:]], axis = 0)\n",
        "      finished_samples += 1\n",
        "  keypoint_set = tf.reshape(keypoint_set, [keypoint_set.shape[0], -1])\n",
        "  return image_set, keypoint_set, finished_samples\n",
        "\n",
        "\n",
        "\n",
        "def get_samples_new(frames_path, labels_folder, batch_size, only_one_sample=True, video_index=0, frame_index=0):\n",
        "  # frames_path = file path to downsampled folder of the video frames\n",
        "  # labels_folder = file path to keypoints folder of the all the pickle lables\n",
        "  # image_dims = (height, width, channel) of an image\n",
        "  # batch_size = number of training examples per training pass\n",
        "  # bundle_size = Number of frames we take to group together to run as a time series\n",
        "\n",
        "  image_height = 270\n",
        "  image_width = 480\n",
        "  image_channel = 3\n",
        "\n",
        "  # image_height, image_width, image_channel = image_dims\n",
        "  video_names = os.listdir(frames_path) # frames path contains folders named with each video containing the frames for that video\n",
        "  result_array = np.zeros(shape=(batch_size, image_height, image_width, image_channel))\n",
        "  labels_array = np.zeros(shape=(batch_size, 51))\n",
        "  # labels_array_notvec = np.zeros(shape=(batch_size, 17,3))\n",
        "\n",
        "  # If we only care about getting a single random sample, then just pick from the list randomly\n",
        "  if only_one_sample:\n",
        "    video = np.random.choice(video_names)\n",
        "  # If we want to pick repeatedly without replacement, then pick up from given video index and frame index\n",
        "  else:\n",
        "    video = video_names[video_index]\n",
        "    \n",
        "  frames_per_video = frames_path + video # path to the directory containing the frames of a given video\n",
        "  frames = os.listdir(frames_per_video) # Gets the list of videonameframe000x.jpg'\n",
        "  num_frames = len(frames)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    # If the frame index exceeds the number of frames left\n",
        "    # while frame_index >= num_frames:  #original line of code\n",
        "    while frame_index >= num_frames - 2: #hacky line of code to account for mismatched labels and frames\n",
        "      # Reset variables for next video\n",
        "      video_index += 1\n",
        "      # Handles the case of running out of videos (returns None)\n",
        "      try:\n",
        "        video = video_names[video_index]\n",
        "      except:\n",
        "        return None\n",
        "      frames_per_video = frames_path + video # path to the directory containing the frames of a given video\n",
        "      frames = os.listdir(frames_per_video) # Gets the list of videonameframe000x.jpg's\n",
        "      frame_index = 0     # Clear frame index\n",
        "      num_frames = len(frames)\n",
        "    # print('frame index:', frame_index, \"num_frames:\", num_frames)\n",
        "\n",
        "    # Create an empty tensor for the each training example\n",
        "    # frame = frames[frame_index] #original line of code\n",
        "    frame = frames[frame_index ] #hacky line of code to account for mismatched labels and frames\n",
        "    image = imread(frames_per_video + \"/\" + frame)\n",
        "\n",
        "    result_array[i, :, :, :] = image/255  # Set inner array to outer array\n",
        "\n",
        "    #pickle stuff\n",
        "    pickle_file = vidpath2picklepath(labels_folder, frames_per_video) \n",
        "    keypoints_tensor = pklpath2tensor(pickle_file)\n",
        "    # print(keypoints_tensor.shape)\n",
        "    keypoint_set = keypoints_tensor[frame_index - 1, :, :]\n",
        "    label = tf.reshape(keypoint_set, [-1])\n",
        "    labels_array[i, :] = label\n",
        "    # labels_array_notvec[i,:,:] = keypoint_set\n",
        "    \n",
        "    frame_index += 1  # Increment the frame index for the next bundle of frames\n",
        "\n",
        "    # Convert to tensor from np array\n",
        "    result_tensor = tf.convert_to_tensor(result_array, dtype=tf.float32)\n",
        "    labels_tensor = tf.convert_to_tensor(labels_array, dtype=tf.float32)\n",
        "\n",
        "  # return result_tensor, labels_array, video_index, frame_index\n",
        "  return result_tensor, labels_tensor, video_index, frame_index\n",
        "\n",
        "\n",
        "######### Model ###############\n",
        "def create_model(lossfunc = 'mae', optimizer = 'adam', metric = 'mae'):\n",
        "  initializer = tf.keras.initializers.GlorotNormal()\n",
        "  model = models.Sequential()\n",
        "\n",
        "\n",
        "\n",
        "  model.add(layers.Conv2D(32, (1, 1), activation='relu', kernel_initializer=initializer, name = 'layer1', input_shape=(270, 480, 3)))\n",
        "  model.add(layers.MaxPooling2D((2, 2), name = 'layer2'))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=initializer, name = 'layer3'))\n",
        "  model.add(layers.MaxPooling2D((2, 2), name = 'layer4'))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=initializer, name = 'layer5'))\n",
        "  model.add(layers.MaxPooling2D((2, 2), name = 'layer6'))\n",
        "  model.add(layers.Conv2D(64, (5, 5), activation='relu', kernel_initializer=initializer, name = 'layer7'))\n",
        "  model.add(layers.MaxPooling2D((3, 3), name = 'layer8'))\n",
        "  model.add(layers.Conv2D(16, (1, 1), activation='relu', kernel_initializer=initializer, name = 'layer9'))\n",
        "  model.add(layers.Flatten(name = 'layer10')) #flatten into 1D vector\n",
        "  initializer_dense = tf.keras.initializers.GlorotNormal()\n",
        "  model.add(layers.Dense(51, kernel_initializer=initializer, name = 'layer11')) \n",
        "\n",
        "  model.compile(optimizer=optimizer,loss= lossfunc,metrics=[metric])\n",
        "  \n",
        "\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hs3vcXW1y14",
        "cellView": "form"
      },
      "source": [
        "#@title Update Model function pulling from  h5py \n",
        "def update_model_h5(num_vids_in_train_set = 5, num_vids_in_test_set = 2, epochs = 5, lossfunc = 'mae', optimizer = 'adam', metric = 'mae', weights_path = 'latest_update'):\n",
        "  print('\\n\\n')\n",
        "  print(\"Model Updating\")\n",
        "  checkpoint_path = f\"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_{datetime.now()}.ckpt\"\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "  # Create a callback that saves the model's weights every 5 epochs\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_path, \n",
        "      verbose=1, \n",
        "      save_weights_only=True,\n",
        "      save_freq='epoch',\n",
        "      save_best_only=True)\n",
        "\n",
        "\n",
        "  #### Fit model\n",
        "  model = create_model(lossfunc, optimizer , metric)\n",
        "\n",
        "  if weights_path == 'latest_update':\n",
        "    latest_update = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    model.load_weights(latest_update)\n",
        "  else:\n",
        "    model.load_weights(weights_path)\n",
        "\n",
        "\n",
        "  #get data\n",
        "  h5py_path = '/content/drive/My Drive/CS230Project/data/MEGADATAFILE.hdf5'\n",
        "  labels_folder = '/content/drive/My Drive/CS230Project/pkl_files/aist_plusplus_final/keypoints3d/'\n",
        "\n",
        "  #get from file\n",
        "  with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/h5_index_file.csv') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      for row in csv_reader:\n",
        "          finished_samples = int(row[0])\n",
        "  print('start_samplenum:', finished_samples)\n",
        "\n",
        "  train_images, train_labels, finished_samples = get_samples_h5py(h5py_path, num_vids_in_train_set, finished_samples)\n",
        "  test_images, test_labels, finished_samples = get_samples_h5py(h5py_path, num_vids_in_test_set, finished_samples)\n",
        "\n",
        "  # #### manually reset start indices if you so desire. dont do this unless you have a good reason\n",
        "  with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/h5_index_file.csv', mode='w') as csv_file:\n",
        "    writer = csv.writer(csv_file, delimiter=',')\n",
        "    writer.writerow([finished_samples])  \n",
        "\n",
        "  print('end_samplenum:', finished_samples)\n",
        "\n",
        "  # Train the model with the new callback\n",
        "  history = model.fit(train_images, \n",
        "            train_labels,\n",
        "            epochs=epochs, \n",
        "            callbacks=[cp_callback],\n",
        "            validation_data=(test_images, test_labels),\n",
        "            verbose=0)\n",
        "  return model, history, train_images, train_labels, test_images, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyY2R6ZEzK-S"
      },
      "source": [
        "#@title Global variabl for persistent test set from video 7500 index 0 400 samples\n",
        "downsampled_frames_folder = \"/content/drive/My Drive/CS230Project/downsampled/\"\n",
        "labels_folder = '/content/drive/My Drive/CS230Project/pkl_files/aist_plusplus_final/keypoints3d/'\n",
        "test_images_persistent, test_labels_persistent, _, _ = get_samples_new(downsampled_frames_folder, labels_folder, 15, False, 5006, 0)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pGnRDUue0xbK",
        "outputId": "12c151ec-eaa9-404d-9671-b35d64db568f"
      },
      "source": [
        "#@title recover best path and loss\n",
        "# '/content/drive/My Drive/CS230Project/checkpoints/regular_1000ex_mae6'\n",
        "model = create_model(lossfunc = 'mae', optimizer = 'adam' , metric = 'mae')\n",
        "default_path = '/content/drive/My Drive/CS230Project/checkpoints/regular_1000ex_mae6'\n",
        "checkpoint_path = f\"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_auto_{datetime.now()}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "auto_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      best_path, best_loss = row[0], float(row[1])\n",
        "\n",
        "path_testing = auto_path\n",
        "\n",
        "model.load_weights(path_testing)\n",
        "loss, acc = model.evaluate(test_images_persistent, test_labels_persistent, verbose=2)\n",
        "with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv', mode='w') as csv_file:\n",
        "  writer = csv.writer(csv_file, delimiter=',')\n",
        "  writer.writerow([path_testing, loss  ]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 - 12s - loss: 18.5246 - mae: 18.5246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw4L0j8wUN07",
        "cellView": "form"
      },
      "source": [
        "#@title Update model function pulling from image folders ####################################################\n",
        "\n",
        "def update_model(num_train_frames = 200, num_test_frames = 5, epochs = 5, lossfunc = 'mae', optimizer = 'adam', metric = 'mae', weights_path = 'best_manual_update'):\n",
        "  print('\\n\\n')\n",
        "  print(\"Model Updating\")\n",
        "  checkpoint_path = f\"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_auto_{datetime.now()}.ckpt\"\n",
        "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "  #Create a callback that saves the model's weights every 5 epochs\n",
        "  cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=checkpoint_path, \n",
        "      verbose=1, \n",
        "      save_weights_only=True,\n",
        "      save_freq='epoch',\n",
        "      save_best_only=True)\n",
        "\n",
        "\n",
        "  #### Fit model\n",
        "  model = create_model(lossfunc, optimizer , metric)\n",
        "\n",
        "\n",
        "  with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      best_path, best_loss = row[0], float(row[1])\n",
        "\n",
        "  if weights_path == 'best_auto_update':\n",
        "    load_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "  elif weights_path == 'best_manual_update':\n",
        "    load_path = best_path\n",
        "    load_path = \"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_manual_2021-03-16 18:40:32.893846\"\n",
        "    # /content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_manual_2021-06-01 06:37:19.052223\n",
        "  else:\n",
        "    load_path = weights_path\n",
        "\n",
        "  model.load_weights(load_path)\n",
        "  print(\"model loaded from\", load_path)\n",
        "\n",
        "\n",
        "  #get data\n",
        "  downsampled_frames_folder = \"/content/drive/My Drive/CS230Project/downsampled/\"\n",
        "  labels_folder = '/content/drive/My Drive/CS230Project/pkl_files/aist_plusplus_final/keypoints3d/'\n",
        "\n",
        "  #get from file\n",
        "  with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/current_index_file.csv') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      for row in csv_reader:\n",
        "          vid_idx, frame_idx = int(row[0]), int(row[1])\n",
        "  \n",
        "  print('start_indices:', vid_idx, frame_idx)\n",
        "  #test and train on same data because theres a separate test set outside of this function\n",
        "  train_images, train_labels, vid_idx, frame_idx = get_samples_new(downsampled_frames_folder, labels_folder, num_train_frames, False, vid_idx, frame_idx)\n",
        "  test_images, test_labels, vid_idx, frame_idx = train_images, train_labels, vid_idx, frame_idx#get_samples_new(downsampled_frames_folder, labels_folder, num_test_frames, False,  vid_idx, frame_idx)\n",
        "\n",
        "  with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/current_index_file.csv', mode='w') as csv_file:\n",
        "      writer = csv.writer(csv_file, delimiter=',')\n",
        "      writer.writerow([vid_idx, frame_idx])\n",
        "\n",
        "  print('end_indices:', vid_idx, frame_idx)\n",
        "\n",
        "  # Train the model with the new callback\n",
        "  history = model.fit(train_images, \n",
        "            train_labels,\n",
        "            epochs=epochs, \n",
        "            # callbacks=[cp_callback],\n",
        "            validation_data=(test_images, test_labels),\n",
        "            verbose=True)\n",
        "  \n",
        "  loss, acc = model.evaluate(test_images_persistent, test_labels_persistent, verbose=2)\n",
        "\n",
        "\n",
        "  \n",
        "  if loss < best_loss:\n",
        "    print(f\"loss on test set of {loss} improves on previous loss of {best_loss}\")\n",
        "    best_loss = loss\n",
        "    best_path = f\"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_manual_{datetime.now()}\"\n",
        "    model.save_weights(best_path)\n",
        "    print(f\"new best model stored in\", best_path)\n",
        "\n",
        "    with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv', mode='w') as csv_file:\n",
        "      writer = csv.writer(csv_file, delimiter=',')\n",
        "      writer.writerow([best_path, best_loss])  \n",
        "  else:\n",
        "    print(f\"loss on test set of {loss} does not improve on previous loss of {best_loss}. Model not manually saved\") \n",
        "\n",
        "  return model, history, train_images, train_labels, test_images, test_labels\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zO3RAtIV-Nr",
        "cellView": "form"
      },
      "source": [
        "#@title Reset best path and loss variables in order to reset training\n",
        "\n",
        "best_path = 'null'\n",
        "best_loss = 100\n",
        "with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv', mode='w') as csv_file:\n",
        "      writer = csv.writer(csv_file, delimiter=',')\n",
        "      writer.writerow([best_path, best_loss])  "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpN_68yH872Z"
      },
      "source": [
        "# hyperparameters\n",
        "lossfunc = 'mae'\n",
        "metric = 'mae'\n",
        "num_train_frames = 15\n",
        "num_test_frames = 15\n",
        "epochs = 30\n",
        "loop_iter = 1\n",
        "weights_path = 'best_auto_update'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQGTMCtfVPqO"
      },
      "source": [
        "##@title Update model pulling images from files ###################################################\n",
        "\n",
        "\n",
        "for k in range(loop_iter): \n",
        "  ################## run model ######################\n",
        "\n",
        "  manual_reset_vid_index = 3065 + k\n",
        "  manual_reset_frame_index = 0\n",
        "  with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/current_index_file.csv', mode='w') as csv_file:\n",
        "    writer = csv.writer(csv_file, delimiter=',')\n",
        "    writer.writerow([manual_reset_vid_index, manual_reset_frame_index])   \n",
        "\n",
        "  start = time.time()   \n",
        "\n",
        "  model, history, train_images, train_labels, test_images, test_labels = update_model( num_train_frames = num_train_frames,  num_test_frames = num_test_frames, epochs = epochs, lossfunc = lossfunc, metric = metric, weights_path = weights_path)\n",
        "  \n",
        "\n",
        "  # end time\n",
        "  end = time.time()\n",
        "\n",
        "  # total time taken\n",
        "  print(f\"Runtime of the program is {end - start}\")\n",
        "\n",
        "  ################# plot model history ###############\n",
        "  plt.figure()\n",
        "  plt.plot(history.history[metric], label=metric)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend(loc='lower right') \n",
        "\n",
        "\n",
        "  # ################### plot 2d projection of individual samples vs training set #################\n",
        "  # predicted_y = model.predict(test_images)\n",
        "  # output_pts = predicted_y.reshape((predicted_y.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "  # label_pts = test_labels.numpy().reshape((test_labels.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "  # print('plots of training data')\n",
        "  # for i in range(num_test_frames):\n",
        "  #     # i = int(test_images.shape[0]/2)\n",
        "  #     print(f'train set loop {k} fig {i}')\n",
        "  #     plt.figure()\n",
        "  #     plt.scatter(output_pts[i,:,0],output_pts[i,:,1], color = 'red')\n",
        "  #     plt.scatter(label_pts[i,:,0],label_pts[i,:,1], color = 'blue')\n",
        "  #     plt.title('Conv2d sample output keypoints. Model predictions in blue, Ground truth in red')\n",
        "  #     plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aySoqWKsUM-D"
      },
      "source": [
        "################### plot 2d projection of individual samples vs persistent test set #################\n",
        "optimizer = 'adam'\n",
        "model = create_model(lossfunc, optimizer , metric)\n",
        "checkpoint_path = f\"/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_auto_{datetime.now()}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "load_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      best_path, best_loss = row[0], float(row[1])\n",
        "\n",
        "\n",
        "load_path = best_path\n",
        "print('loaded from:',load_path)\n",
        "model.load_weights(load_path)\n",
        "\n",
        "start = time.time()\n",
        "predicted_y = model.predict(test_images_persistent)\n",
        "end = time.time()\n",
        "print(f\"Runtime of the program is {end - start}\")\n",
        "\n",
        "output_pts = predicted_y.reshape((predicted_y.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "label_pts = test_labels_persistent.numpy().reshape((test_labels_persistent.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "print('plots of test data')\n",
        "for i in range(num_test_frames):\n",
        "    # i = int(test_images.shape[0]/2)\n",
        "    print(f'test set loop {k} fig {i}')\n",
        "    plt.figure()\n",
        "    plt.scatter(output_pts[i,:,0],output_pts[i,:,1], color = 'blue')\n",
        "    plt.scatter(label_pts[i,:,0],label_pts[i,:,1], color = 'red')\n",
        "    plt.title('Conv2d sample output keypoints. Model predictions in blue, Ground truth in red')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdXhdcNw8qN1",
        "cellView": "form"
      },
      "source": [
        "#@title h5py hyperparameters and default weight reload (these are together for no reason)\n",
        "lossfunc = 'mae'\n",
        "metric = 'mae'\n",
        "num_vids_in_train_set = 5\n",
        "num_vids_in_test_set = 1\n",
        "epochs = 5\n",
        "loop_iter = 10\n",
        "\n",
        "# '/content/drive/My Drive/CS230Project/checkpoints/regular_1000ex_mae6'\n",
        "model = create_model(lossfunc = 'mae', optimizer = 'adam' , metric = 'mae')\n",
        "default_path = '/content/drive/My Drive/CS230Project/checkpoints/regular_1000ex_mae6'\n",
        "model.load_weights(default_path)\n",
        "loss, acc = model.evaluate(test_images_persistent, test_labels_persistent, verbose=2)\n",
        "with open('/content/drive/MyDrive/CS230Project/best_stuff/best_stuff.csv', mode='w') as csv_file:\n",
        "  writer = csv.writer(csv_file, delimiter=',')\n",
        "  writer.writerow([default_path, loss  ]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GlMZNEzJ8Ljd"
      },
      "source": [
        "#@title Update model pulling images from h5py ###################################################\n",
        "h5 = True\n",
        "for i in range(loop_iter): \n",
        "  ################## run model ######################\n",
        "  if h5:\n",
        "    model, history, train_images, train_labels, test_images, test_labels = update_model_h5(num_vids_in_train_set = num_vids_in_train_set, num_vids_in_test_set = num_vids_in_test_set, epochs = epochs, lossfunc = lossfunc, metric = metric)#, weights_path = '/content/drive/MyDrive/CS230Project/regular_model_checkpoints/cp_2021-03-16 03:02:02.611619.ckpt')\n",
        "  else:\n",
        "    model, history, train_images, train_labels, test_images, test_labels = update_model( num_train_frames = num_train_frames,  num_test_frames = num_test_frames, epochs = epochs, lossfunc = lossfunc, metric = metric)\n",
        "  \n",
        "  ################# plot model history ###############\n",
        "  plt.plot(history.history[metric], label=metric)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend(loc='lower right') \n",
        "\n",
        "\n",
        "  ################### plot 2d projection of individual samples vs test set #################\n",
        "  predicted_y = model.predict(test_images)\n",
        "  output_pts = predicted_y.reshape((predicted_y.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "  label_pts = test_labels.numpy().reshape((test_labels.shape[0],17,int(predicted_y.shape[1]/17)))\n",
        "\n",
        "  for i in range(1):\n",
        "      plt.figure()\n",
        "      plt.scatter(output_pts[i,:,0],output_pts[i,:,1], color = 'red')\n",
        "      plt.scatter(label_pts[i,:,0],label_pts[i,:,1], color = 'blue')\n",
        "      plt.title('Sample output plot. Model predictions in blue, Ground truth in red')\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "179-ZAoSXQpo"
      },
      "source": [
        "#@title plot model history ###############\n",
        "plt.plot(history.history[metric], label=metric)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel(metric)\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wyFGuYN2X5TW"
      },
      "source": [
        "#@title plot individual samples vs test set #################\n",
        "predicted_y = model.predict(train_images)\n",
        "output_pts = predicted_y.reshape((predicted_y.shape[0],17,3))\n",
        "label_pts = test_labels.numpy().reshape((test_labels.shape[0],17,3))\n",
        "\n",
        "#2d plots of outputs and labels of selected images \n",
        "for i in range(1):\n",
        "    i = 2\n",
        "    plt.figure()\n",
        "    plt.scatter(output_pts[i,:,0],output_pts[i,:,1], color = 'red')\n",
        "    plt.scatter(label_pts[i,:,0],label_pts[i,:,1], color = 'blue')\n",
        "    plt.title('Sample output plot. Model predictions in blue, Ground truth in red')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F9SYhO6YlLk",
        "cellView": "form"
      },
      "source": [
        "#@title  manually reset start indices if you so desire. dont do this unless you have a good reason\n",
        "manual_reset_vid_index = 5007\n",
        "manual_reset_frame_index = 50\n",
        "with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/current_index_file.csv', mode='w') as csv_file:\n",
        "    writer = csv.writer(csv_file, delimiter=',')\n",
        "    writer.writerow([manual_reset_vid_index, manual_reset_frame_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XanA5ZEA2gAT"
      },
      "source": [
        "#@title h5py manually reset vid_number if you so desire. dont do this unless you have a good reason\n",
        "manual_num_vids = 48\n",
        "with open('/content/drive/MyDrive/CS230Project/regular_model_checkpoints/indices/h5_index_file.csv', mode='w') as csv_file:\n",
        "    writer = csv.writer(csv_file, delimiter=',')\n",
        "    writer.writerow([manual_num_vids])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}